# -*- coding: utf-8 -*-
"""vgg19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mpdmdm-UC59xhWLrV8xRWUNIh1Owti95
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Load Packages"""

pip install scikit-plot

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from keras.utils import np_utils
import scikitplot

"""# Load Dataframe"""

train_df = pd.read_csv('/content/drive/MyDrive/EMOCNN/fer2013.csv')

train_df.head()

train_df.shape

"""# Label Distribution"""

(train_df.emotion.value_counts() / len(train_df)).to_frame().sort_index().T

sns.countplot(x='emotion',data=train_df)
plt.show()

"""# View Sample of Images"""

emotion_label = {0: 'Anger', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}

fig = plt.figure(1, (14,14))
k=0

for label in sorted(train_df.emotion.unique()):
    for j in range(7):
        px = train_df[train_df.emotion==label].pixels.iloc[k]
        px = np.array(px.split(' ')).reshape(48,48).astype('float32')
        
        k += 1
        ax = plt.subplot(7,7,k)
        ax.imshow(px)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(emotion_label[label])
        plt.tight_layout()

"""# Data Generators"""

train_df['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in train_df['pixels']]
img_array = np.concatenate(train_df.pixels)

le = LabelEncoder()
#One-hit-encoded labels
img_labels = le.fit_transform(train_df.emotion)
img_labels = np_utils.to_categorical(img_labels)

print('Pixel Shape: ', img_array.shape)
print('Label Shape: ', img_labels.shape)

X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels, shuffle=True, stratify=img_labels, test_size=0.2, random_state=1)

print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)

Xs_train = X_train / 255
Xs_valid = X_valid / 255

#Used for Model
X_train_color = np.repeat(Xs_train, repeats=3, axis=3)
X_valid_color = np.repeat(Xs_valid, repeats=3, axis=3)
print(X_train_color.shape)
print(X_valid_color.shape)

"""# Build Network"""

base_model = tf.keras.applications.VGG19(input_shape=(48,48,3),
                                    include_top=False,
                                    weights='imagenet')

#Start with a non trainable model
base_model.trainable = False

base_model.summary()

cnn = Sequential ([
    base_model,
    
    Flatten(),
    
    Dense(128, activation='relu'),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.4),
    BatchNormalization(),
    Dense(7, activation='softmax')
])

cnn.summary()

"""# Train Network"""

opt = tf.keras.optimizers.Adam(0.001)

cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

"""## Training Run 1"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# h1 = cnn.fit(
#     X_train_color,y_train,
#     steps_per_epoch=len(X_train_color) / 64,
#     epochs=500,
#     validation_data=(X_valid_color,y_valid),
#     validation_steps=len(X_valid_color) / 64,
#     verbose=1
# )

history = h1.history
print(history.keys())

epoch_range = range(1, len(history['loss'])+1)

plt.figure(figsize=[14,4])
plt.subplot(1,2,1)
plt.plot(epoch_range, history['loss'], label='Training')
plt.plot(epoch_range, history['val_loss'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(epoch_range, history['accuracy'], label='Training')
plt.plot(epoch_range, history['val_accuracy'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

cnn.save('/content/drive/MyDrive/EMOCNN/vgg19.h5')
pickle.dump(history, open(f'/content/drive/MyDrive/EMOCNN/vgg19.pkl', 'wb'))

"""## Training Run 2"""

tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# h2 = cnn.fit(
#     X_train_color,y_train,
#     steps_per_epoch=len(X_train_color) / 64,
#     epochs=50,
#     validation_data=(X_valid_color,y_valid),
#     validation_steps=len(X_valid_color) / 64,
#     verbose=1
# )

for k in history.keys():
    history[k] += h2.history[k]

epoch_range = range(1, len(history['loss'])+1)

plt.figure(figsize=[14,4])
plt.subplot(1,2,1)
plt.plot(epoch_range, history['loss'], label='Training')
plt.plot(epoch_range, history['val_loss'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(epoch_range, history['accuracy'], label='Training')
plt.plot(epoch_range, history['val_accuracy'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

"""## Traing Run 3"""

base_model.trainable = True

opt = tf.keras.optimizers.Adam(0.00001)
cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# h3 = cnn.fit(
#     X_train_color,y_train,
#     steps_per_epoch=len(X_train_color) / 64,
#     epochs=50,
#     validation_data=(X_valid_color,y_valid),
#     validation_steps=len(X_valid_color) / 64,
#     verbose=1
# )

for k in history.keys():
    history[k] += h3.history[k]

epoch_range = range(1, len(history['loss'])+1)

plt.figure(figsize=[14,4])
plt.subplot(1,2,1)
plt.plot(epoch_range, history['loss'], label='Training')
plt.plot(epoch_range, history['val_loss'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')
plt.legend()
plt.subplot(1,2,2)
plt.plot(epoch_range, history['accuracy'], label='Training')
plt.plot(epoch_range, history['val_accuracy'], label='Validation')
plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

"""## Training Run 4"""

opt = tf.keras.optimizers.Adam(0.000001)
cnn.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# h4 = cnn.fit(
#     X_train_color,y_train,
#     steps_per_epoch=len(X_train_color) / 64,
#     epochs=50,
#     validation_data=(X_valid_color,y_valid),
#     validation_steps=len(X_valid_color) / 64,
#     verbose=1
# )

"""# Save Model"""

cnn.save('FER_Transfer_Learning_v02.h5')
pickle.dump(history, open(f'FER_Transfer_Learning_v02.pkl', 'wb'))